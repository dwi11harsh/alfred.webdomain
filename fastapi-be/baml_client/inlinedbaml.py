# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "ai-calls/framework.baml": "enum Framework {\n    React\n    Node\n}\n\nfunction GetFramework (prompt: string) -> Framework {\n    client Gpt4ominiSmall\n    prompt #\"\n        {{_.role(\"system\")}}\n        respond with a suitable framework from the listed framework as to which one would be suitable for the user's requirment\n        ---\n        {{_.role(\"user\")}}: {{prompt}}\n        ---\n\n        {{ctx.output_format}}\n\n        Response\n\n    \"#\n}\n\ntest Test1 {\n    functions [GetFramework]\n        args {\n            prompt \"Build a web app for my todo application\"\n        }\n}",
    "ai-calls/nodejs/node_generation_steps.baml": "enum ComponentType {\n    Route\n    Middleware\n    Utility\n    Configuration\n    Model\n    Service\n}\n\nclass ProjectComponent {\n    type ComponentType\n    name string\n    description string\n    dependencies string[] @description(\"List of other components this depends on\")\n    specific_instructions string @description(\"Step-by-step instructions for implementation\")\n    expected_behavior string @description(\"Exactly what this component should do\")\n    input_validation string @description(\"If applicable, validation rules for inputs, return empty string if its not needed\")\n    error_handling string @description(\"How errors should be handled\")\n    example_usage string @description(\"Example of how this would be used, return empty string if its not needed\")\n}\n\nclass ProjectStructure {\n    components ProjectComponent[]\n    entry_point string @description(\"Main file that ties everything together\")\n    package_dependencies string[] @description(\"Only include dependencies mentioned in user prompt\")\n    file_structure string[] @description(\"List of files that need to be created\")\n    implementation_order string[] @description(\"Order in which components should be implemented\")\n}\n\nfunction PlanExpressServer (user_prompt:string) -> ProjectStructure {\n    client Gpt4omini\n    \n    prompt #\"\n        {{_.role(\"system\")}}\n        You are an expert Node.js/Express/TypeScript architect. Your job is to break down the user's Express server requirements into atomic, unambiguous components that can be implemented independently by smaller AI models to write clean TypeScript codes.\n\n        CRITICAL CONSTRAINTS:\n        1. Each component must be self-contained with clear boundaries\n        2. Instructions must be explicit and leave no room for interpretation\n        3. Components should be ordered by dependencies\n        4. Only include what the user explicitly requested\n        5. Assume the smaller model has basic Express knowledge but will follow instructions literally\n        6. Each entry needs to be a standalone prompt that does not need any extra context to to generate required output\n\n        BREAKDOWN STRATEGY:\n        - Routes: Define exact HTTP methods, paths, request/response formats\n        - Middleware: Specify exact functionality and order\n        - Utilities: Isolate helper functions with exact signatures\n        - Configuration: Specify exact settings and environment variables\n        - Always include error handling specifications\n        - Always include input validation requirements\n\n        {{_.role(\"user\")}}\n        Create an Express server with the following requirements:\n        {{user_prompt}}\n\n        Please break this down into a structured plan that ensures consistent, production-ready output from smaller AI models.\n\n        {{ctx.output_format}}\n\n        Response:\n    \"#\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> Gpt4ominiSmall {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n    temperature 0.1\n    max_tokens 50\n  }\n}\n\nclient<llm> Gpt4omini {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n    temperature 0.7\n  }\n}\n\nclient<llm> Gpt5miniLimited {\n  provider openai\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n  }\n}\n\nclient<llm> Gpt5 {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n  }\n}",
    "config.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.213.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
}

def get_baml_files():
    return _file_map