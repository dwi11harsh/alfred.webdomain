# ----------------------------------------------------------------------------
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml
#
# ----------------------------------------------------------------------------

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code using: baml-cli generate
# baml-cli is available with the baml package.

_file_map = {

    "ai-calls/framework.baml": "enum Framework {\n    React\n    Node\n}\n\nfunction GetFramework (prompt: string) -> Framework {\n    client Gpt4omini\n    prompt #\"\n        {{_.role(\"system\")}}\n        respond with a suitable framework from the listed framework as to which one would be suitable for the user's requirment\n        ---\n        {{_.role(\"user\")}}: {{prompt}}\n        ---\n\n        {{ctx.output_format}}\n\n        Response\n\n    \"#\n}\n\ntest Test1 {\n    functions [GetFramework]\n        args {\n            prompt \"Build a web app for my todo application\"\n        }\n}",
    "ai-calls/nodejs/node_generation_steps.baml": "enum DefinitionType {\n    Route\n    HelperFunction\n    Middleware\n}\n\nclass File {\n    route string @description(#\"\n        name of route. this may or may not include parameters depending upon the need\n    \"#)\n    input string @description(#\"\n        the input that the defined route\n    \"#)\n    output string @description(#\"\n        output of defined route\n    \"#)\n    process string @description(#\"\n        how will this function convert the input into required output\n    \"#)\n}\n\n\nfunction PlanNodeProject (prompt:string) -> File[] {\n    client Gpt5\n\n    prompt #\"\n        {{_.role(\"system\")}}\n\n        Return an array of {{File}} that contains prompts to create an ExpressJS backend for the user's prompt\n        These prompts will then be fed to a smaller model to create a fully fledged, production ready backend server\n        Don't add the support for any external library, databases, middlewares, auth etc. until and unless specified by the user\n        The final output from these prompts needs to be a single index.ts file that contains all the logics for the database\n        Add support for multiple files only if user explicitely asks for it\n\n        ---\n        {{_.role(\"user\")}}\n\n        {{prompt}}\n\n        ---\n\n        {{ctx.output_format}}\n\n        Response:\n    \"#\n}",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> Gpt4omini {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n    temperature 0.1\n    max_tokens 50\n  }\n}\n\nclient<llm> Gpt5mini {\n  provider openai\n  options {\n    model \"gpt-5-mini\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n    temperature 0.1\n    max_tokens 1000\n  }\n}\n\nclient<llm> Gpt5 {\n  provider openai\n  options {\n    model \"gpt-5\"\n    api_key env.OPENAI_API_KEY\n    base_url env.OPENAI_API_BASE\n  }\n}",
    "config.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.213.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode async\n}\n",
}

def get_baml_files():
    return _file_map